{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bhargav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Bhargav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bhargav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Bhargav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Bhargav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- ------------\n",
      "asttokens         2.4.1\n",
      "beautifulsoup4    4.12.3\n",
      "bs4               0.0.2\n",
      "certifi           2024.2.2\n",
      "chardet           3.0.4\n",
      "click             8.1.7\n",
      "colorama          0.4.6\n",
      "comm              0.2.0\n",
      "contourpy         1.2.0\n",
      "cycler            0.12.1\n",
      "debugpy           1.8.0\n",
      "decorator         5.1.1\n",
      "executing         2.0.1\n",
      "fonttools         4.48.1\n",
      "googletrans       3.0.0\n",
      "h11               0.9.0\n",
      "h2                3.2.0\n",
      "hpack             3.0.0\n",
      "hstspreload       2024.2.1\n",
      "httpcore          0.9.1\n",
      "httpx             0.13.3\n",
      "hyperframe        5.2.0\n",
      "idna              2.10\n",
      "ipykernel         6.27.1\n",
      "ipython           8.18.1\n",
      "jedi              0.19.1\n",
      "joblib            1.3.2\n",
      "jupyter_client    8.6.0\n",
      "jupyter_core      5.5.0\n",
      "kiwisolver        1.4.5\n",
      "matplotlib        3.8.2\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.5.8\n",
      "nltk              3.8.1\n",
      "numpy             1.26.4\n",
      "packaging         23.2\n",
      "pandas            2.2.0\n",
      "parso             0.8.3\n",
      "pillow            10.2.0\n",
      "pip               24.0\n",
      "platformdirs      4.0.0\n",
      "prompt-toolkit    3.0.41\n",
      "psutil            5.9.6\n",
      "pure-eval         0.2.2\n",
      "PyAudio           0.2.14\n",
      "pygame            2.5.2\n",
      "Pygments          2.17.2\n",
      "pyparsing         3.1.1\n",
      "python-dateutil   2.8.2\n",
      "pytz              2024.1\n",
      "pywin32           306\n",
      "pyzmq             25.1.1\n",
      "regex             2023.12.25\n",
      "rfc3986           1.5.0\n",
      "scikit-learn      1.4.0\n",
      "scipy             1.12.0\n",
      "seaborn           0.13.2\n",
      "setuptools        69.0.2\n",
      "six               1.16.0\n",
      "sniffio           1.3.0\n",
      "soupsieve         2.5\n",
      "stack-data        0.6.3\n",
      "textblob          0.18.0.post0\n",
      "threadpoolctl     3.2.0\n",
      "tornado           6.4\n",
      "tqdm              4.66.2\n",
      "traitlets         5.14.0\n",
      "tzdata            2024.1\n",
      "wcwidth           0.2.12\n",
      "wheel             0.42.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent ='they told  that their ages are 21 25 and 23 respectively.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average age is: 23.0\n"
     ]
    }
   ],
   "source": [
    "# find the average of ages mentioned in the sentence\n",
    "sent = 'they told  that their ages are 21, 25 and 23 respectively.'\n",
    "\n",
    "# Extract ages\n",
    "ages_str = sent.split()[-3:-1]  # Extract the last two words before 'respectively'\n",
    "\n",
    "# Convert the ages to numericals\n",
    "ages_int = [int(age) for age in ages_str if age.isdigit()]\n",
    "\n",
    "# sum of the ages\n",
    "ages = sum(ages_int)\n",
    "\n",
    "# Find the average\n",
    "average = ages / len(ages_int)\n",
    "\n",
    "print(\"The average age is:\", average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_int = [int(age) for age in ages_str if age.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ages_int)/len(ages_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= 'Hello friends! How are you? I hope ,you like Python Programming.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'I hope ,you like Python Programming.']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " ',',\n",
       " 'you',\n",
       " 'like',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### punctuation symbol removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for word in word_tokenize(sent):\n",
    "    if word not in string.punctuation:\n",
    "        new.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'like',\n",
       " 'Python',\n",
       " 'Programming']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello friends How are you I hope you like Python Programming\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = ' '.join(new)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ' '.join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello friends How are you I hope you like Python Programming'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "swords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean=[]\n",
    "for word in word_tokenize(result):\n",
    "        if word.lower() not in swords:\n",
    "            clean.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'friends', 'hope', 'like', 'Python', 'Programming']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Porter  Stemmer\n",
    "words1='plays','player','players','playing','played'\n",
    "words2='go','gone','going','went'\n",
    "words3='association','associated,','associate','associating'\n",
    "words4='bad','worse','worst','better'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('plays', 'player', 'players', 'playing', 'played')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'play'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps=PorterStemmer()\n",
    "ps.stem(words1[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(words2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "player\n",
      "player\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "for word in words1:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "gone\n",
      "go\n",
      "went\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for word in words2:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "associ\n",
      "associated,\n",
      "associ\n",
      "associ\n"
     ]
    }
   ],
   "source": [
    "for word in words3:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "wors\n",
      "worst\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "for word in words4:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "play\n",
      "play\n",
      "play\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls=LancasterStemmer()\n",
    "for word in words1:\n",
    "    print (ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "gon\n",
      "going\n",
      "went\n"
     ]
    }
   ],
   "source": [
    "for word in words2:\n",
    "    print(ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assocy\n",
      "associated,\n",
      "assocy\n",
      "assocy\n"
     ]
    }
   ],
   "source": [
    "for word in words3:\n",
    "    print(ls.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "wors\n",
      "worst\n",
      "bet\n"
     ]
    }
   ],
   "source": [
    "for word in words4:\n",
    "    print(ls.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: player\n",
      "Verb: players\n",
      "Adjective: players\n",
      "Adverb: players\n"
     ]
    }
   ],
   "source": [
    "words='players'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: working\n",
      "Verb: work\n",
      "Adjective: working\n",
      "Adverb: working\n"
     ]
    }
   ],
   "source": [
    "words='working'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: went\n",
      "Verb: go\n",
      "Adjective: went\n",
      "Adverb: went\n"
     ]
    }
   ],
   "source": [
    "words='went'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: bottle\n",
      "Verb: bottle\n",
      "Adjective: bottle\n",
      "Adverb: bottle\n"
     ]
    }
   ],
   "source": [
    "words='bottle'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: switched\n",
      "Verb: switch\n",
      "Adjective: switched\n",
      "Adverb: switched\n"
     ]
    }
   ],
   "source": [
    "words='switched'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: worst\n",
      "Verb: worst\n",
      "Adjective: bad\n",
      "Adverb: worst\n"
     ]
    }
   ],
   "source": [
    "words='worst'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: child\n",
      "Verb: children\n",
      "Adjective: children\n",
      "Adverb: children\n"
     ]
    }
   ],
   "source": [
    "words='children'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: ate\n",
      "Verb: eat\n",
      "Adjective: ate\n",
      "Adverb: ate\n"
     ]
    }
   ],
   "source": [
    "words='ate'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun: spoke\n",
      "Verb: speak\n",
      "Adjective: spoke\n",
      "Adverb: spoke\n"
     ]
    }
   ],
   "source": [
    "words='spoke'\n",
    "print('Noun:',wnl.lemmatize(words,pos='n'))\n",
    "print('Verb:',wnl.lemmatize(words,pos='v'))\n",
    "print('Adjective:',wnl.lemmatize(words,pos='a'))\n",
    "print('Adverb:',wnl.lemmatize(words,pos='r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= 'Hello friends! How are you? I hope ,you like Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=pos_tag(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('friends', 'VBZ'),\n",
       " ('!', '.'),\n",
       " ('How', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('?', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('hope', 'VBP'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('like', 'VBP'),\n",
       " ('Python', 'NNP'),\n",
       " ('Programming', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word, tags in tags:\n",
    "    if tags.startswith(word):\n",
    "     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
